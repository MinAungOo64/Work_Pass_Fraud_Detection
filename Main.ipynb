{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9d61d2",
   "metadata": {},
   "source": [
    "# Data Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d98dd",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "08c9e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# -------------------------\n",
    "# Config knobs you can tweak\n",
    "# -------------------------\n",
    "N_EMPLOYERS = 300          # number of companies applying for work passes\n",
    "N_WORKERS = 2000           # unique foreign workers in pool\n",
    "N_APPLICATIONS = 20000      # total work pass applications (rows)\n",
    "FRAUD_RATE = 0.04          # ~4% labelled suspicious\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "DAYS_SPAN = 60             # simulate 2 months of activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c117e8",
   "metadata": {},
   "source": [
    "## 1. Generate entity profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4fa179cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employers: assign each a sector and \"typical salary band\"\n",
    "sectors = [\"Construction\", \"Marine\", \"F&B\", \"Manufacturing\", \"Logistics\", \"Cleaning\"]\n",
    "employer_ids = np.arange(10000, 10000 + N_EMPLOYERS)\n",
    "\n",
    "employer_df = pd.DataFrame({\n",
    "    \"employer_id\": employer_ids,\n",
    "    \"sector\": rng.choice(sectors, size=N_EMPLOYERS, replace=True),\n",
    "    # base_salary_band is like their usual declared monthly salary for foreign hires\n",
    "    \"base_salary_band\": rng.normal(loc=2800, scale=600, size=N_EMPLOYERS).clip(1600, 6000),\n",
    "    # normal application volume per 30 days\n",
    "    \"baseline_applications_per_month\": rng.poisson(lam=4, size=N_EMPLOYERS) + 1\n",
    "})\n",
    "\n",
    "# Workers (foreign hires) - assign nationality and \"expected wage tier\"\n",
    "countries = [\"CN\",\"IN\",\"BD\",\"PH\",\"MM\",\"TH\",\"ID\",\"MY\",\"LK\",\"VN\"]\n",
    "worker_ids = np.arange(500000, 500000 + N_WORKERS)\n",
    "\n",
    "worker_df = pd.DataFrame({\n",
    "    \"worker_id\": worker_ids,\n",
    "    \"nationality\": rng.choice(countries, size=N_WORKERS, replace=True),\n",
    "    # skill_tier 1-3 affects expected salary (e.g. Tier 3 = higher skill, higher pay)\n",
    "    \"skill_tier\": rng.integers(low=1, high=4, size=N_WORKERS)\n",
    "})\n",
    "\n",
    "# helper: expected salary by skill_tier\n",
    "tier_salary_map = {\n",
    "    1: (2200, 400),   # mean, std\n",
    "    2: (3000, 500),\n",
    "    3: (4200, 800),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bc493",
   "metadata": {},
   "source": [
    "## 2. Generate work pass applications (transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8efffbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>employer_id</th>\n",
       "      <th>sector</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>nationality</th>\n",
       "      <th>skill_tier</th>\n",
       "      <th>declared_salary</th>\n",
       "      <th>est_monthly_load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10530</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>10163</td>\n",
       "      <td>F&amp;B</td>\n",
       "      <td>500460</td>\n",
       "      <td>VN</td>\n",
       "      <td>1</td>\n",
       "      <td>2842.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15452</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>10264</td>\n",
       "      <td>Cleaning</td>\n",
       "      <td>500507</td>\n",
       "      <td>MM</td>\n",
       "      <td>2</td>\n",
       "      <td>2689.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4927</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>10287</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>501511</td>\n",
       "      <td>BD</td>\n",
       "      <td>3</td>\n",
       "      <td>4413.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15872</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>10232</td>\n",
       "      <td>Construction</td>\n",
       "      <td>500748</td>\n",
       "      <td>MY</td>\n",
       "      <td>2</td>\n",
       "      <td>3096.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4293</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>10040</td>\n",
       "      <td>Construction</td>\n",
       "      <td>500646</td>\n",
       "      <td>MY</td>\n",
       "      <td>1</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   application_id  timestamp  employer_id         sector  worker_id  \\\n",
       "0           10530 2025-01-01        10163            F&B     500460   \n",
       "1           15452 2025-01-01        10264       Cleaning     500507   \n",
       "2            4927 2025-01-01        10287  Manufacturing     501511   \n",
       "3           15872 2025-01-01        10232   Construction     500748   \n",
       "4            4293 2025-01-01        10040   Construction     500646   \n",
       "\n",
       "  nationality  skill_tier  declared_salary  est_monthly_load  \n",
       "0          VN           1           2842.0                11  \n",
       "1          MM           2           2689.0                 9  \n",
       "2          BD           3           4413.0                 5  \n",
       "3          MY           2           3096.0                 8  \n",
       "4          MY           1           2230.0                 9  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# 2. Generate work pass applications (transactions)\n",
    "# =========================\n",
    "# We'll sample applications: (employer, worker, timestamp, declared salary, etc.)\n",
    "\n",
    "def random_application(i):\n",
    "    # pick employer\n",
    "    emp = employer_df.sample(1, weights=None, random_state=rng.integers(0, 10**9)).iloc[0]\n",
    "    emp_id = emp.employer_id\n",
    "\n",
    "    # pick worker\n",
    "    wrk = worker_df.sample(1, random_state=rng.integers(0, 10**9)).iloc[0]\n",
    "    wrk_id = wrk.worker_id\n",
    "\n",
    "    # timestamp within window\n",
    "    ts_offset_days = rng.integers(0, DAYS_SPAN)\n",
    "    ts_offset_hours = rng.integers(0, 24)\n",
    "    ts = START_DATE + timedelta(days=int(ts_offset_days), hours=int(ts_offset_hours))\n",
    "\n",
    "    # \"clean\" (non-fraud) salary proposal normally tied to worker tier & employer band\n",
    "    tier_mean, tier_std = tier_salary_map[wrk.skill_tier]\n",
    "    base = (tier_mean + emp.base_salary_band) / 2   # average both expectations\n",
    "    declared_salary_clean = rng.normal(loc=base, scale=(tier_std+200)/2)\n",
    "\n",
    "    # Slight noise\n",
    "    declared_salary_clean = np.clip(declared_salary_clean, 1600, 8000)\n",
    "\n",
    "    # number of prior applications by this employer in last 30 days\n",
    "    # (we approximate using employer baseline + Poisson jitter)\n",
    "    recent_load_est = emp.baseline_applications_per_month + rng.poisson(2)\n",
    "\n",
    "    return {\n",
    "        \"application_id\": i,\n",
    "        \"timestamp\": ts,\n",
    "        \"employer_id\": emp_id,\n",
    "        \"sector\": emp.sector,\n",
    "        \"worker_id\": wrk_id,\n",
    "        \"nationality\": wrk.nationality,\n",
    "        \"skill_tier\": wrk.skill_tier,\n",
    "        \"declared_salary\": round(declared_salary_clean, 0),\n",
    "        \"est_monthly_load\": int(recent_load_est)\n",
    "    }\n",
    "\n",
    "applications = [random_application(i) for i in range(N_APPLICATIONS)]\n",
    "df = pd.DataFrame(applications)\n",
    "df.sort_values(\"timestamp\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829f9aa",
   "metadata": {},
   "source": [
    "## 3. Inject suspicious patterns (fraud logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c6e379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final suspicious rate: 0.4744\n",
      "Final suspicious rate (after proper downsampling): 0.03\n",
      "Total applications after downsampling: 10837\n"
     ]
    }
   ],
   "source": [
    "# We'll mark some rows as suspicious_flag = 1 using rules that MOM-style fraud teams care about:\n",
    "#   Pattern A: Salary too low for claimed skill_tier\n",
    "#   Pattern B: Sudden application spike by employer\n",
    "#   Pattern C: High churn: same worker_id appearing with multiple different employers quickly\n",
    "\n",
    "df[\"suspicious_flag\"] = 0\n",
    "\n",
    "# Pattern A: underpayment relative to tier\n",
    "def expected_min_pay(tier):\n",
    "    # super rough \"legal/market floor\" concept\n",
    "    if tier == 1: return 1800\n",
    "    if tier == 2: return 2600\n",
    "    return 3500  # tier 3\n",
    "underpaid_mask = df.apply(\n",
    "    lambda row: row.declared_salary < expected_min_pay(row.skill_tier) * 0.7,\n",
    "    axis=1\n",
    ")\n",
    "# What is loc? It's a way to access a group of rows and columns by labels or a boolean array.\n",
    "# df[underpaid_mask, \"suspicious_flag\"] = 1 # Does this code work? Answer: No, it should be df.loc[underpaid_mask, \"suspicious_flag\"] = 1\n",
    "df.loc[underpaid_mask, \"suspicious_flag\"] = 1\n",
    "\n",
    "# Pattern B: spike in est_monthly_load (possible quota abuse / pass farming)\n",
    "spike_mask = df[\"est_monthly_load\"] > (df[\"est_monthly_load\"].median() + 3*df[\"est_monthly_load\"].std())\n",
    "df.loc[spike_mask, \"suspicious_flag\"] = 1\n",
    "\n",
    "# Pattern C: worker hopping employers in <4 days\n",
    "# first, get worker timeline\n",
    "df[\"timestamp_unix\"] = df[\"timestamp\"].astype(\"int64\") // 10**9  # seconds\n",
    "df = df.sort_values([\"worker_id\",\"timestamp_unix\"])\n",
    "df[\"prev_employer\"] = df.groupby(\"worker_id\")[\"employer_id\"].shift(1)\n",
    "df[\"prev_time\"] = df.groupby(\"worker_id\")[\"timestamp_unix\"].shift(1)\n",
    "rapid_reapply_mask = (\n",
    "    (df[\"prev_employer\"].notna()) &\n",
    "    (df[\"prev_employer\"] != df[\"employer_id\"]) &\n",
    "    ((df[\"timestamp_unix\"] - df[\"prev_time\"]) < 4*24*3600)  # < 4 days\n",
    ")\n",
    "df.loc[rapid_reapply_mask, \"suspicious_flag\"] = 1\n",
    "\n",
    "# Clean up helper cols for presentation\n",
    "df.drop(columns=[\"timestamp_unix\",\"prev_employer\",\"prev_time\"], inplace=True)\n",
    "\n",
    "df[\"suspicious_flag\"] = df[\"suspicious_flag\"].astype(int)\n",
    "\n",
    "fraud_rate_actual = df[\"suspicious_flag\"].mean()\n",
    "print(\"Final suspicious rate:\", round(fraud_rate_actual,4))\n",
    "\n",
    "## Downsample suspicious rows to target fraud rate\n",
    "target_rate = 0.03  # e.g. 3%\n",
    "\n",
    "# Count how many are currently fraud / not fraud\n",
    "fraud_mask = df[\"suspicious_flag\"] == 1\n",
    "N_fraud_orig = fraud_mask.sum()\n",
    "N_norm_orig = (~fraud_mask).sum()\n",
    "\n",
    "# desired fraud rows to keep\n",
    "k = int((target_rate / (1 - target_rate)) * N_norm_orig)\n",
    "\n",
    "# when more fraudulent rows than desired, downsample\n",
    "if N_fraud_orig > k:\n",
    "    fraud_indices_to_keep = (\n",
    "        df[fraud_mask]\n",
    "        .sample(n=k, random_state=42)\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    df = pd.concat([\n",
    "        df[~fraud_mask],            # keep all normal\n",
    "        df.loc[fraud_indices_to_keep]  # keep only k fraud\n",
    "    ]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# sanity check\n",
    "final_rate = df[\"suspicious_flag\"].mean()\n",
    "print(\"Final suspicious rate (after proper downsampling):\", round(final_rate, 4))\n",
    "print(\"Total applications after downsampling:\", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3a904aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.403361862391694"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(underpaid_mask)\n",
    "df[\"est_monthly_load\"].median() + 3*df[\"est_monthly_load\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32966071",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "98f51c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"synthetic_workpass_fraud.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdc9f5",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e6963",
   "metadata": {},
   "source": [
    "## Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "15cc522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      " application_id        int64\n",
      "timestamp            object\n",
      "employer_id           int64\n",
      "sector               object\n",
      "worker_id             int64\n",
      "nationality          object\n",
      "skill_tier            int64\n",
      "declared_salary     float64\n",
      "est_monthly_load      int64\n",
      "suspicious_flag       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"synthetic_workpass_fraud.csv\")\n",
    "print(\"\\nData types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775eb3d",
   "metadata": {},
   "source": [
    "## Convert Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0fb898d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_id               int64\n",
      "timestamp           datetime64[ns]\n",
      "employer_id                  int64\n",
      "sector                      object\n",
      "worker_id                    int64\n",
      "nationality                 object\n",
      "skill_tier                   int64\n",
      "declared_salary            float64\n",
      "est_monthly_load             int64\n",
      "suspicious_flag              int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c816a0",
   "metadata": {},
   "source": [
    "## Create New Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef66753",
   "metadata": {},
   "source": [
    "Based on our synthetic data generation rules for fraud patterns, we can create new columns to help identify suspicious applications.  \n",
    "Pattern A: Underpaid applicants (declared_salary significantly below expected for skill_tier)  \n",
    "Pattern B: Spike in est_monthly_load (possible quota abuse / pass farming)  \n",
    "Pattern C: Frequent changes in employment history (job hopping)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dc6b8d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e0979790\\AppData\\Local\\Temp\\ipykernel_4632\\3341683101.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"days_since_last_application\"].fillna(999, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>employer_id</th>\n",
       "      <th>sector</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>nationality</th>\n",
       "      <th>skill_tier</th>\n",
       "      <th>declared_salary</th>\n",
       "      <th>est_monthly_load</th>\n",
       "      <th>suspicious_flag</th>\n",
       "      <th>tier_mean_salary</th>\n",
       "      <th>salary_deviation</th>\n",
       "      <th>tier_salary_ratio</th>\n",
       "      <th>load_zscore</th>\n",
       "      <th>prev_application_time</th>\n",
       "      <th>days_since_last_application</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>10016</td>\n",
       "      <td>2025-01-07 12:00:00</td>\n",
       "      <td>10003</td>\n",
       "      <td>F&amp;B</td>\n",
       "      <td>500000</td>\n",
       "      <td>VN</td>\n",
       "      <td>1</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2498.716310</td>\n",
       "      <td>-296.716310</td>\n",
       "      <td>0.881253</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>NaT</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10441</th>\n",
       "      <td>2180</td>\n",
       "      <td>2025-01-13 14:00:00</td>\n",
       "      <td>10170</td>\n",
       "      <td>Construction</td>\n",
       "      <td>500001</td>\n",
       "      <td>ID</td>\n",
       "      <td>3</td>\n",
       "      <td>4559.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3556.790250</td>\n",
       "      <td>1002.209750</td>\n",
       "      <td>1.281774</td>\n",
       "      <td>-0.809657</td>\n",
       "      <td>NaT</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10155</th>\n",
       "      <td>10640</td>\n",
       "      <td>2025-01-01 10:00:00</td>\n",
       "      <td>10218</td>\n",
       "      <td>F&amp;B</td>\n",
       "      <td>500002</td>\n",
       "      <td>MM</td>\n",
       "      <td>2</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2904.632302</td>\n",
       "      <td>-1038.632302</td>\n",
       "      <td>0.642422</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>NaT</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>7001</td>\n",
       "      <td>2025-01-02 00:00:00</td>\n",
       "      <td>10072</td>\n",
       "      <td>F&amp;B</td>\n",
       "      <td>500003</td>\n",
       "      <td>MM</td>\n",
       "      <td>3</td>\n",
       "      <td>3662.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3556.790250</td>\n",
       "      <td>105.209750</td>\n",
       "      <td>1.029580</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>NaT</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>11675</td>\n",
       "      <td>2025-01-05 06:00:00</td>\n",
       "      <td>10192</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>500004</td>\n",
       "      <td>ID</td>\n",
       "      <td>3</td>\n",
       "      <td>2479.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3556.790250</td>\n",
       "      <td>-1077.790250</td>\n",
       "      <td>0.696977</td>\n",
       "      <td>-0.404436</td>\n",
       "      <td>NaT</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       application_id           timestamp  employer_id         sector  \\\n",
       "1545            10016 2025-01-07 12:00:00        10003            F&B   \n",
       "10441            2180 2025-01-13 14:00:00        10170   Construction   \n",
       "10155           10640 2025-01-01 10:00:00        10218            F&B   \n",
       "5333             7001 2025-01-02 00:00:00        10072            F&B   \n",
       "9202            11675 2025-01-05 06:00:00        10192  Manufacturing   \n",
       "\n",
       "       worker_id nationality  skill_tier  declared_salary  est_monthly_load  \\\n",
       "1545      500000          VN           1           2202.0                 7   \n",
       "10441     500001          ID           3           4559.0                 5   \n",
       "10155     500002          MM           2           1866.0                 7   \n",
       "5333      500003          MM           3           3662.0                 7   \n",
       "9202      500004          ID           3           2479.0                 6   \n",
       "\n",
       "       suspicious_flag  tier_mean_salary  salary_deviation  tier_salary_ratio  \\\n",
       "1545                 0       2498.716310       -296.716310           0.881253   \n",
       "10441                0       3556.790250       1002.209750           1.281774   \n",
       "10155                0       2904.632302      -1038.632302           0.642422   \n",
       "5333                 0       3556.790250        105.209750           1.029580   \n",
       "9202                 0       3556.790250      -1077.790250           0.696977   \n",
       "\n",
       "       load_zscore prev_application_time  days_since_last_application  \n",
       "1545      0.000785                   NaT                        999.0  \n",
       "10441    -0.809657                   NaT                        999.0  \n",
       "10155     0.000785                   NaT                        999.0  \n",
       "5333      0.000785                   NaT                        999.0  \n",
       "9202     -0.404436                   NaT                        999.0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pattern A: underpayment relative to tier\n",
    "# Find mean pay for each skill tier\n",
    "df[\"tier_mean_salary\"] = df.groupby(\"skill_tier\")[\"declared_salary\"].transform(\"mean\")\n",
    "# Calculate salary deviation from tier mean\n",
    "df[\"salary_deviation\"] = df[\"declared_salary\"] - df[\"tier_mean_salary\"]\n",
    "# Create tier_salary_ratio feature, if low ratio indicates underpayment\n",
    "df[\"tier_salary_ratio\"] = df[\"declared_salary\"] / df[\"tier_mean_salary\"]\n",
    "\n",
    "# Pattern B: spike in est_monthly_load (possible quota abuse / pass farming)\n",
    "# Calculate z-score for est_monthly_load \n",
    "df[\"load_zscore\"] = (df[\"est_monthly_load\"] - df[\"est_monthly_load\"].mean()) / df[\"est_monthly_load\"].std()\n",
    "\n",
    "# Pattern C: worker hopping employers in <4 days\n",
    "# Find previous application time for each worker\n",
    "df = df.sort_values([\"worker_id\",\"timestamp\"])\n",
    "df[\"prev_application_time\"] = df.groupby(\"worker_id\")[\"timestamp\"].shift(1)\n",
    "# Calculate time difference in days since last application\n",
    "df[\"days_since_last_application\"] = (df[\"timestamp\"] - df[\"prev_application_time\"]).dt.days\n",
    "# If first application, fill NaN with large number\n",
    "df[\"days_since_last_application\"].fillna(999, inplace=True)\n",
    "\n",
    "# Sanity check for first applications\n",
    "df[df[\"days_since_last_application\"]==999].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5871e62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>employer_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>skill_tier</th>\n",
       "      <th>declared_salary</th>\n",
       "      <th>est_monthly_load</th>\n",
       "      <th>suspicious_flag</th>\n",
       "      <th>tier_mean_salary</th>\n",
       "      <th>salary_deviation</th>\n",
       "      <th>...</th>\n",
       "      <th>nationality_BD</th>\n",
       "      <th>nationality_CN</th>\n",
       "      <th>nationality_ID</th>\n",
       "      <th>nationality_IN</th>\n",
       "      <th>nationality_LK</th>\n",
       "      <th>nationality_MM</th>\n",
       "      <th>nationality_MY</th>\n",
       "      <th>nationality_PH</th>\n",
       "      <th>nationality_TH</th>\n",
       "      <th>nationality_VN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>10016</td>\n",
       "      <td>2025-01-07 12:00:00</td>\n",
       "      <td>10003</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2498.71631</td>\n",
       "      <td>-296.71631</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9235</th>\n",
       "      <td>8218</td>\n",
       "      <td>2025-01-15 13:00:00</td>\n",
       "      <td>10186</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2498.71631</td>\n",
       "      <td>-479.71631</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6413</th>\n",
       "      <td>14445</td>\n",
       "      <td>2025-01-21 15:00:00</td>\n",
       "      <td>10274</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "      <td>2640.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2498.71631</td>\n",
       "      <td>141.28369</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>5479</td>\n",
       "      <td>2025-02-06 18:00:00</td>\n",
       "      <td>10298</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "      <td>2882.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2498.71631</td>\n",
       "      <td>383.28369</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>8978</td>\n",
       "      <td>2025-02-08 02:00:00</td>\n",
       "      <td>10177</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2498.71631</td>\n",
       "      <td>21.28369</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      application_id           timestamp  employer_id  worker_id  skill_tier  \\\n",
       "1545           10016 2025-01-07 12:00:00        10003     500000           1   \n",
       "9235            8218 2025-01-15 13:00:00        10186     500000           1   \n",
       "6413           14445 2025-01-21 15:00:00        10274     500000           1   \n",
       "575             5479 2025-02-06 18:00:00        10298     500000           1   \n",
       "1934            8978 2025-02-08 02:00:00        10177     500000           1   \n",
       "\n",
       "      declared_salary  est_monthly_load  suspicious_flag  tier_mean_salary  \\\n",
       "1545           2202.0                 7                0        2498.71631   \n",
       "9235           2019.0                 5                0        2498.71631   \n",
       "6413           2640.0                 5                0        2498.71631   \n",
       "575            2882.0                 6                0        2498.71631   \n",
       "1934           2520.0                 7                1        2498.71631   \n",
       "\n",
       "      salary_deviation  ...  nationality_BD  nationality_CN nationality_ID  \\\n",
       "1545        -296.71631  ...           False           False          False   \n",
       "9235        -479.71631  ...           False           False          False   \n",
       "6413         141.28369  ...           False           False          False   \n",
       "575          383.28369  ...           False           False          False   \n",
       "1934          21.28369  ...           False           False          False   \n",
       "\n",
       "      nationality_IN  nationality_LK  nationality_MM  nationality_MY  \\\n",
       "1545           False           False           False           False   \n",
       "9235           False           False           False           False   \n",
       "6413           False           False           False           False   \n",
       "575            False           False           False           False   \n",
       "1934           False           False           False           False   \n",
       "\n",
       "      nationality_PH  nationality_TH  nationality_VN  \n",
       "1545           False           False            True  \n",
       "9235           False           False            True  \n",
       "6413           False           False            True  \n",
       "575            False           False            True  \n",
       "1934           False           False            True  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=[\"sector\", \"nationality\"])\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d431ffb3",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51049bfc",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d1ad7",
   "metadata": {},
   "source": [
    "First we fit a general logistic regression model to see which predictors are most important in identifying fraudulent applications. We can expect the engineered features to be significant predictors of fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6b5b1a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.127170\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:        suspicious_flag   No. Observations:                10837\n",
      "Model:                          Logit   Df Residuals:                    10816\n",
      "Method:                           MLE   Df Model:                           20\n",
      "Date:                Tue, 28 Oct 2025   Pseudo R-squ.:                 0.05595\n",
      "Time:                        20:49:22   Log-Likelihood:                -1378.1\n",
      "converged:                       True   LL-Null:                       -1459.8\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.695e-24\n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "const                           0.5172   3.07e+06   1.68e-07      1.000   -6.02e+06    6.02e+06\n",
      "declared_salary                -0.0015      0.001     -1.579      0.114      -0.003       0.000\n",
      "est_monthly_load                0.5412        nan        nan        nan         nan         nan\n",
      "skill_tier                      0.8775      0.511      1.717      0.086      -0.124       1.879\n",
      "salary_deviation                0.0028      0.001      2.108      0.035       0.000       0.005\n",
      "tier_salary_ratio              -4.9361      2.565     -1.925      0.054      -9.962       0.090\n",
      "load_zscore                    -1.2472        nan        nan        nan         nan         nan\n",
      "days_since_last_application    -0.0047      0.001     -4.032      0.000      -0.007      -0.002\n",
      "sector_Cleaning                 0.0090        nan        nan        nan         nan         nan\n",
      "sector_Construction            -0.0379        nan        nan        nan         nan         nan\n",
      "sector_F&B                     -0.0513        nan        nan        nan         nan         nan\n",
      "sector_Logistics                0.0891        nan        nan        nan         nan         nan\n",
      "sector_Manufacturing            0.1666        nan        nan        nan         nan         nan\n",
      "sector_Marine                   0.3417        nan        nan        nan         nan         nan\n",
      "nationality_BD                  0.3346        nan        nan        nan         nan         nan\n",
      "nationality_CN                 -0.0847        nan        nan        nan         nan         nan\n",
      "nationality_ID                 -0.3740        nan        nan        nan         nan         nan\n",
      "nationality_IN                  0.1028        nan        nan        nan         nan         nan\n",
      "nationality_LK                 -0.1373        nan        nan        nan         nan         nan\n",
      "nationality_MM                  0.0047        nan        nan        nan         nan         nan\n",
      "nationality_MY                  0.3866        nan        nan        nan         nan         nan\n",
      "nationality_PH                 -0.0066        nan        nan        nan         nan         nan\n",
      "nationality_TH                  0.0741        nan        nan        nan         nan         nan\n",
      "nationality_VN                  0.2171        nan        nan        nan         nan         nan\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fit a general logistic regression model to see which predictors are most important in identifying fraudulent applications. We can expect the engineered features to be significant predictors of fraud.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Prepare features and target\n",
    "feature_cols = [\n",
    "    \"declared_salary\",\n",
    "    \"est_monthly_load\",\n",
    "    \"skill_tier\",\n",
    "    \"salary_deviation\",\n",
    "    \"tier_salary_ratio\",\n",
    "    \"load_zscore\",\n",
    "    \"days_since_last_application\"\n",
    "] + [col for col in df_encoded.columns if col.startswith(\"sector_\") or col.startswith(\"nationality_\")] \n",
    "X = df_encoded[feature_cols].astype(float)\n",
    "y = df_encoded[\"suspicious_flag\"].astype(int)\n",
    "\n",
    "# fit logistic regression without standardization\n",
    "logit_model = sm.Logit(y, sm.add_constant(X))\n",
    "result = logit_model.fit()\n",
    "print(result.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e436f",
   "metadata": {},
   "source": [
    "As predicted, nationality and sector dummies are not significant predictors of fraud, while our engineered features show strong significance. Due to perfect correlation between load_zscore and est_monthly_load, we drop load_zscore from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "92b4affc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.128045\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:        suspicious_flag   No. Observations:                10837\n",
      "Model:                          Logit   Df Residuals:                    10830\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Tue, 28 Oct 2025   Pseudo R-squ.:                 0.04945\n",
      "Time:                        22:07:37   Log-Likelihood:                -1387.6\n",
      "converged:                       True   LL-Null:                       -1459.8\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.186e-28\n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "const                           4.2356      3.205      1.322      0.186      -2.046      10.517\n",
      "declared_salary                -0.0016      0.001     -1.623      0.105      -0.003       0.000\n",
      "est_monthly_load                0.0348      0.023      1.520      0.128      -0.010       0.080\n",
      "skill_tier                      0.8947      0.510      1.754      0.079      -0.105       1.895\n",
      "salary_deviation                0.0028      0.001      2.130      0.033       0.000       0.005\n",
      "tier_salary_ratio              -4.8658      2.549     -1.909      0.056      -9.863       0.131\n",
      "days_since_last_application    -0.0047      0.001     -4.023      0.000      -0.007      -0.002\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"declared_salary\",\n",
    "    \"est_monthly_load\",\n",
    "    \"skill_tier\",\n",
    "    \"salary_deviation\",\n",
    "    \"tier_salary_ratio\",\n",
    "    # \"load_zscore\",\n",
    "    \"days_since_last_application\"\n",
    "]\n",
    "X = df_encoded[feature_cols].astype(float)\n",
    "y = df_encoded[\"suspicious_flag\"].astype(int)\n",
    "\n",
    "# fit logistic regression without standardization\n",
    "logit_model = sm.Logit(y, sm.add_constant(X))\n",
    "result = logit_model.fit()\n",
    "print(result.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a7ed2",
   "metadata": {},
   "source": [
    "Type II Error (missed fraud): 0.142  \n",
    "Type I Error (false alarms): 0.641   \n",
    "ROC AUC: 0.665  \n",
    "\n",
    "While the Type II error is acceptable, the high Type I error indicates many false alarms. Further model tuning and feature engineering may be needed to improve precision in fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f1037c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.858\n",
      "Type II Error (missed fraud): 0.142\n",
      "Type I Error (false alarms): 0.641\n",
      "ROC AUC: 0.665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "logit = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "logit.fit(X, y)\n",
    "y_pred = logit.predict(X)\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "type2_error = fn / (tp + fn)   # 1 - recall\n",
    "type1_error = fp / (fp + tn)   # 1 - specificity\n",
    "\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"Type II Error (missed fraud): {type2_error:.3f}\")\n",
    "print(f\"Type I Error (false alarms): {type1_error:.3f}\")\n",
    "\n",
    "# ROC AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_prob = logit.predict_proba(X)[:, 1]\n",
    "roc_auc = roc_auc_score(y, y_prob)\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c591a8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating at threshold = 0.3\n",
      "Recall: 0.991\n",
      "Type II Error (missed fraud): 0.009\n",
      "Type I Error (false alarms): 0.810\n",
      "ROC AUC: 0.657\n",
      "\n",
      "Evaluating at threshold = 0.5\n",
      "Recall: 0.837\n",
      "Type II Error (missed fraud): 0.163\n",
      "Type I Error (false alarms): 0.634\n",
      "ROC AUC: 0.657\n",
      "\n",
      "Evaluating at threshold = 0.7\n",
      "Recall: 0.003\n",
      "Type II Error (missed fraud): 0.997\n",
      "Type I Error (false alarms): 0.000\n",
      "ROC AUC: 0.657\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_lr_cv(X, y, n_splits=5, threshold=0.5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    recalls = []\n",
    "    type2_errors = []\n",
    "    type1_errors = []\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # train weighted logistic regression\n",
    "        logit = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "        logit.fit(X_train, y_train)\n",
    "\n",
    "        # predicted probs on test fold\n",
    "        y_prob = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # convert to hard labels using threshold\n",
    "        y_pred = (y_prob > threshold).astype(int)\n",
    "\n",
    "        # confusion matrix on test fold\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        type2_error = fn / (tp + fn) if (tp + fn) > 0 else 0.0      # 1 - recall\n",
    "        type1_error = fp / (fp + tn) if (fp + tn) > 0 else 0.0      # FPR\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        recalls.append(recall)\n",
    "        type2_errors.append(type2_error)\n",
    "        type1_errors.append(type1_error)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    print(f\"Recall: {np.mean(recalls):.3f}\")\n",
    "    print(f\"Type II Error (missed fraud): {np.mean(type2_errors):.3f}\")\n",
    "    print(f\"Type I Error (false alarms): {np.mean(type1_errors):.3f}\")\n",
    "    print(f\"ROC AUC: {np.mean(aucs):.3f}\")\n",
    "\n",
    "# Evaluate for different thresholds\n",
    "for thresh in [0.3, 0.5, 0.7]:\n",
    "    print(f\"\\nEvaluating at threshold = {thresh}\")\n",
    "    evaluate_lr_cv(X, y, n_splits=5, threshold=thresh)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90f7a9",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a3fda",
   "metadata": {},
   "source": [
    "Evaluating Decision Tree at threshold = 0.26  \n",
    "Recall: 0.898  \n",
    "Type II Error (missed fraud): 0.102  \n",
    "Type I Error (false alarms): 0.285  \n",
    "ROC AUC: 0.91  \n",
    "\n",
    "Decision trees hold a high AUC value of 0.917, indicating strong discriminatory power between fraudulent and legitimate applications. Setting the threshold to 0.26 achieves a good balance between catching fraud (high recall) while keeping false alarms relatively low. Further tuning of tree parameters and thresholds can help optimize performance for specific operational needs in fraud detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ec16b1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 6, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "Best ROC-AUC: 0.865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "# Define model\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'min_samples_split': [2, 10, 20, 30],\n",
    "    'min_samples_leaf': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "small_grid = {'max_depth':[4,6], 'min_samples_split':[2,10], 'min_samples_leaf':[1,5]}\n",
    "\n",
    "# Define stratified K-Fold CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Use ROC-AUC as the scoring metric\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Set up grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scorer,\n",
    "    cv=cv\n",
    ")\n",
    "\n",
    "# Run search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "\n",
    "# Print best results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best ROC-AUC: {grid_search.best_score_:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3918689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Decision Tree at threshold = 0.22\n",
      "Recall: 0.923\n",
      "Type II Error (missed fraud): 0.077\n",
      "Type I Error (false alarms): 0.342\n",
      "ROC AUC: 0.917\n",
      "\n",
      "Evaluating Decision Tree at threshold = 0.24\n",
      "Recall: 0.920\n",
      "Type II Error (missed fraud): 0.080\n",
      "Type I Error (false alarms): 0.324\n",
      "ROC AUC: 0.917\n",
      "\n",
      "Evaluating Decision Tree at threshold = 0.26\n",
      "Recall: 0.898\n",
      "Type II Error (missed fraud): 0.102\n",
      "Type I Error (false alarms): 0.285\n",
      "ROC AUC: 0.917\n",
      "\n",
      "Evaluating Decision Tree at threshold = 0.28\n",
      "Recall: 0.892\n",
      "Type II Error (missed fraud): 0.108\n",
      "Type I Error (false alarms): 0.278\n",
      "ROC AUC: 0.917\n",
      "\n",
      "Evaluating Decision Tree at threshold = 0.3\n",
      "Recall: 0.892\n",
      "Type II Error (missed fraud): 0.108\n",
      "Type I Error (false alarms): 0.275\n",
      "ROC AUC: 0.917\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_dt_cv(X, y, n_splits=5, threshold=0.5,\n",
    "                   max_depth=None,\n",
    "                   min_samples_split=2,\n",
    "                   min_samples_leaf=1):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    recalls = []\n",
    "    type2_errors = []\n",
    "    type1_errors = []\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # train weighted decision tree\n",
    "        dt = DecisionTreeClassifier(\n",
    "            class_weight='balanced',\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "        dt.fit(X_train, y_train)\n",
    "\n",
    "        # predicted probs on test fold for the positive class (fraud = 1)\n",
    "        y_prob = dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # convert to hard labels using threshold\n",
    "        y_pred = (y_prob > threshold).astype(int)\n",
    "\n",
    "        # confusion matrix on test fold\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        type2_error = fn / (tp + fn) if (tp + fn) > 0 else 0.0      # miss rate\n",
    "        type1_error = fp / (fp + tn) if (fp + tn) > 0 else 0.0      # false alarm rate\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        recalls.append(recall)\n",
    "        type2_errors.append(type2_error)\n",
    "        type1_errors.append(type1_error)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    print(f\"Recall: {np.mean(recalls):.3f}\")\n",
    "    print(f\"Type II Error (missed fraud): {np.mean(type2_errors):.3f}\")\n",
    "    print(f\"Type I Error (false alarms): {np.mean(type1_errors):.3f}\")\n",
    "    print(f\"ROC AUC: {np.mean(aucs):.3f}\")\n",
    "\n",
    "# Evaluate tree at different decision thresholds\n",
    "for thresh in [0.22, 0.24, 0.26, 0.28, 0.3]:\n",
    "    print(f\"\\nEvaluating Decision Tree at threshold = {thresh}\")\n",
    "    evaluate_dt_cv(\n",
    "        X, y,\n",
    "        n_splits=5,\n",
    "        threshold=thresh,\n",
    "        max_depth=6,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37378db5",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77608fd0",
   "metadata": {},
   "source": [
    "Evaluating Random Forest at threshold = 0.32  \n",
    "Recall: 0.858  \n",
    "Type II Error (missed fraud): 0.142  \n",
    "Type I Error (false alarms): 0.182  \n",
    "ROC AUC: 0.931  \n",
    "\n",
    "Random Forests further improve fraud detection performance, achieving a high AUC of 0.931. At a threshold of 0.32, the model maintains a low Type II error of 0.142 while significantly reducing Type I error to 0.182 compared to decision trees. This indicates fewer false alarms while still effectively identifying fraudulent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "aed3d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "Best Parameters: {'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best ROC-AUC: 0.868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "# Define model\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],          # number of trees\n",
    "    'max_depth': [4, 6, 8, 10],          # tree depth\n",
    "    'min_samples_split': [2, 10, 20],    # min samples to split a node\n",
    "    'min_samples_leaf': [1, 5, 10],      # min samples at a leaf\n",
    "    'max_features': ['sqrt', 'log2']     # number of features per split\n",
    "}\n",
    "\n",
    "# Define stratified K-Fold CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Use ROC-AUC as the scoring metric\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Set up grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,       # parallelize\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print best results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best ROC-AUC: {grid_search.best_score_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "0c20cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Random Forest at threshold = 0.3\n",
      "Recall: 0.902\n",
      "Type II Error (missed fraud): 0.098\n",
      "Type I Error (false alarms): 0.240\n",
      "ROC AUC: 0.931\n",
      "\n",
      "Evaluating Random Forest at threshold = 0.32\n",
      "Recall: 0.858\n",
      "Type II Error (missed fraud): 0.142\n",
      "Type I Error (false alarms): 0.182\n",
      "ROC AUC: 0.931\n",
      "\n",
      "Evaluating Random Forest at threshold = 0.34\n",
      "Recall: 0.825\n",
      "Type II Error (missed fraud): 0.175\n",
      "Type I Error (false alarms): 0.114\n",
      "ROC AUC: 0.931\n",
      "\n",
      "Evaluating Random Forest at threshold = 0.36\n",
      "Recall: 0.766\n",
      "Type II Error (missed fraud): 0.234\n",
      "Type I Error (false alarms): 0.058\n",
      "ROC AUC: 0.931\n",
      "\n",
      "Evaluating Random Forest at threshold = 0.38\n",
      "Recall: 0.754\n",
      "Type II Error (missed fraud): 0.246\n",
      "Type I Error (false alarms): 0.027\n",
      "ROC AUC: 0.931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_rf_cv(X, y,\n",
    "                   n_splits=5,\n",
    "                   threshold=0.5,\n",
    "                   n_estimators=200,\n",
    "                   max_depth=None,\n",
    "                   min_samples_split=2,\n",
    "                   min_samples_leaf=1,\n",
    "                   max_features='sqrt',\n",
    "                   random_state=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    recalls = []\n",
    "    type2_errors = []\n",
    "    type1_errors = []\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # train weighted random forest\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            class_weight='balanced',\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # predicted probs on test fold for the positive class (fraud = 1)\n",
    "        y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # convert to hard labels using chosen threshold\n",
    "        y_pred = (y_prob > threshold).astype(int)\n",
    "\n",
    "        # confusion matrix on test fold\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        type2_error = fn / (tp + fn) if (tp + fn) > 0 else 0.0      # miss rate\n",
    "        type1_error = fp / (fp + tn) if (fp + tn) > 0 else 0.0      # false alarm rate\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        recalls.append(recall)\n",
    "        type2_errors.append(type2_error)\n",
    "        type1_errors.append(type1_error)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    print(f\"Recall: {np.mean(recalls):.3f}\")\n",
    "    print(f\"Type II Error (missed fraud): {np.mean(type2_errors):.3f}\")\n",
    "    print(f\"Type I Error (false alarms): {np.mean(type1_errors):.3f}\")\n",
    "    print(f\"ROC AUC: {np.mean(aucs):.3f}\")\n",
    "\n",
    "# {'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
    "# Sweep thresholds \n",
    "for thresh in [0.3, 0.32, 0.34, 0.36, 0.38]:\n",
    "    print(f\"\\nEvaluating Random Forest at threshold = {thresh}\")\n",
    "    evaluate_rf_cv(\n",
    "        X, y,\n",
    "        n_splits=5,\n",
    "        threshold=thresh,\n",
    "        n_estimators=100,        \n",
    "        max_depth=4,             \n",
    "        min_samples_split=10,    \n",
    "        min_samples_leaf=1,      \n",
    "        max_features='sqrt',     \n",
    "        random_state=42\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d06e2",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5806e",
   "metadata": {},
   "source": [
    "Evaluating CatBoost at threshold = 0.34  \n",
    "Recall: 0.855  \n",
    "Type II Error (missed fraud): 0.145  \n",
    "Type I Error (false alarms): 0.148  \n",
    "ROC AUC: 0.932  \n",
    "CatBoost achieves the highest AUC of 0.932 among the models tested, indicating excellent capability in distinguishing fraudulent from legitimate applications. At a threshold of 0.34, it maintains a low Type II error of 0.145 while further reducing Type I error to 0.148. This balance makes CatBoost a strong candidate for deployment in fraud detection systems, effectively minimizing both missed fraud and false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d44f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Parameters: {'border_count': 64, 'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.05, 'n_estimators': 200}\n",
      "Best ROC-AUC: 0.937\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# custom scorer because older sklearn in your env doesn't support needs_proba in make_scorer\n",
    "def auc_scorer(estimator, X_val, y_val):\n",
    "    # predict_proba returns [:,1] = P(class 1)\n",
    "    y_prob = estimator.predict_proba(X_val)[:, 1]\n",
    "    # guard: if one-class or constant probs, fallback to 0.5\n",
    "    if len(set(y_val)) < 2:\n",
    "        return 0.5\n",
    "    if (y_prob == y_prob[0]).all():\n",
    "        return 0.5\n",
    "    return roc_auc_score(y_val, y_prob)\n",
    "\n",
    "# base model\n",
    "# We'll keep verbose=0 so it doesn't spam during GridSearchCV\n",
    "cat = CatBoostClassifier(\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    # handle class imbalance:\n",
    "    # we can set scale_pos_weight=y_neg/y_pos if you want stronger minority weighting.\n",
    "    # we'll leave it out in the grid for now, can add later.\n",
    ")\n",
    "\n",
    "# parameter grid\n",
    "# Note: CatBoost has many hyperparams; start small or it'll explode in runtime.\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8],          # tree depth (like max_depth)\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [200, 400],  # boosting rounds\n",
    "    'l2_leaf_reg': [1, 5, 10],   # L2 regularization on leaves\n",
    "    'border_count': [32, 64]     # number of splits for numeric features\n",
    "}\n",
    "\n",
    "# stratified CV same style\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=cat,\n",
    "    param_grid=param_grid,\n",
    "    scoring=auc_scorer,   # custom scorer that uses predict_proba\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best ROC-AUC: {grid_search.best_score_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "16c43ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight ≈ 32.34\n",
      "\n",
      "Evaluating CatBoost at threshold = 0.3\n",
      "Recall: 0.871\n",
      "Type II Error (missed fraud): 0.129\n",
      "Type I Error (false alarms): 0.190\n",
      "ROC AUC: 0.932\n",
      "\n",
      "Evaluating CatBoost at threshold = 0.32\n",
      "Recall: 0.865\n",
      "Type II Error (missed fraud): 0.135\n",
      "Type I Error (false alarms): 0.168\n",
      "ROC AUC: 0.932\n",
      "\n",
      "Evaluating CatBoost at threshold = 0.34\n",
      "Recall: 0.855\n",
      "Type II Error (missed fraud): 0.145\n",
      "Type I Error (false alarms): 0.148\n",
      "ROC AUC: 0.932\n",
      "\n",
      "Evaluating CatBoost at threshold = 0.36\n",
      "Recall: 0.846\n",
      "Type II Error (missed fraud): 0.154\n",
      "Type I Error (false alarms): 0.129\n",
      "ROC AUC: 0.932\n",
      "\n",
      "Evaluating CatBoost at threshold = 0.38\n",
      "Recall: 0.834\n",
      "Type II Error (missed fraud): 0.166\n",
      "Type I Error (false alarms): 0.110\n",
      "ROC AUC: 0.932\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_catboost_cv(X, y,\n",
    "                         n_splits=5,\n",
    "                         threshold=0.5,\n",
    "                         depth=6,\n",
    "                         border_count=32,\n",
    "                         learning_rate=0.1,\n",
    "                         n_estimators=200,\n",
    "                         l2_leaf_reg=3,\n",
    "                         random_state=42,\n",
    "                         scale_pos_weight=None):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    recalls, type2_errors, type1_errors, aucs = [], [], [], []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # train CatBoost (silent)\n",
    "        model = CatBoostClassifier(\n",
    "            depth=depth,\n",
    "            border_count=border_count,\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=n_estimators,\n",
    "            l2_leaf_reg=l2_leaf_reg,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # predicted probs on test fold\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # convert to hard labels\n",
    "        y_pred = (y_prob > threshold).astype(int)\n",
    "\n",
    "        # confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        type2_error = fn / (tp + fn) if (tp + fn) > 0 else 0.0      # miss rate\n",
    "        type1_error = fp / (fp + tn) if (fp + tn) > 0 else 0.0      # false alarm rate\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        recalls.append(recall)\n",
    "        type2_errors.append(type2_error)\n",
    "        type1_errors.append(type1_error)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    print(f\"Recall: {np.mean(recalls):.3f}\")\n",
    "    print(f\"Type II Error (missed fraud): {np.mean(type2_errors):.3f}\")\n",
    "    print(f\"Type I Error (false alarms): {np.mean(type1_errors):.3f}\")\n",
    "    print(f\"ROC AUC: {np.mean(aucs):.3f}\")\n",
    "\n",
    "\n",
    "# ---- Run CV across thresholds ----\n",
    "# Compute positive/negative ratio to handle imbalance\n",
    "pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"scale_pos_weight ≈ {pos_weight:.2f}\")\n",
    "\n",
    "# Best Parameters: {'border_count': 64, 'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.05, 'n_estimators': 200}\n",
    "\n",
    "for thresh in [0.3, 0.32, 0.34, 0.36, 0.38]:\n",
    "    print(f\"\\nEvaluating CatBoost at threshold = {thresh}\")\n",
    "    evaluate_catboost_cv(\n",
    "        X, y,\n",
    "        n_splits=5,\n",
    "        threshold=thresh,\n",
    "        depth=4,\n",
    "        border_count=64,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=200,\n",
    "        l2_leaf_reg=1,\n",
    "        scale_pos_weight=pos_weight,  # handles imbalance\n",
    "        random_state=42\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eda3a5",
   "metadata": {},
   "source": [
    "## Unsupervised Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e294d291",
   "metadata": {},
   "source": [
    "KMeans achieve the highest ROC-AUC of 0.663, overall all unsupervised models perform worse than supervised models due to the lack of label information during training. However, they can still provide value in scenarios where labeled data is scarce or unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9e33be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: [10512   325]\n",
      "Resampled dataset shape: [7995 8815]\n"
     ]
    }
   ],
   "source": [
    "# SMOTEEN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "print(\"Original dataset shape:\", np.bincount(y))\n",
    "print(\"Resampled dataset shape:\", np.bincount(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5fed7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "y_true = y_resampled.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7cf34322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud ratio in dataset: 0.5244\n"
     ]
    }
   ],
   "source": [
    "# Percentage of frauds\n",
    "fraud_ratio = y_true.sum() / len(y_true)\n",
    "print(f\"Fraud ratio in dataset: {fraud_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a22adb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Isolation Forest ===\n",
      "Recall (catch frauds): 0.014\n",
      "Precision (accuracy of fraud flags): 0.248\n",
      "F1 score: 0.027\n",
      "ROC AUC: 0.517\n",
      "Confusion matrix:\n",
      "[[7615  380]\n",
      " [8690  125]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# train unsupervised model\n",
    "iso = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.03,   # expected fraction of frauds\n",
    "    random_state=42\n",
    ")\n",
    "iso.fit(X_scaled)\n",
    "\n",
    "# predict anomalies (-1 = anomaly, 1 = normal)\n",
    "y_pred_unsup = iso.predict(X_scaled)\n",
    "\n",
    "# convert to binary 0/1 (fraud=1)\n",
    "y_pred_unsup = np.where(y_pred_unsup == -1, 1, 0)\n",
    "\n",
    "# evaluate against true fraud labels\n",
    "cm = confusion_matrix(y_true, y_pred_unsup)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "# invert\n",
    "auc = roc_auc_score(y_true, -y_pred_unsup)\n",
    "\n",
    "print(\"=== Isolation Forest ===\")\n",
    "print(f\"Recall (catch frauds): {recall:.3f}\")\n",
    "print(f\"Precision (accuracy of fraud flags): {precision:.3f}\")\n",
    "print(f\"F1 score: {f1:.3f}\")\n",
    "print(f\"ROC AUC: {auc:.3f}\")\n",
    "print(f\"Confusion matrix:\\n{cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "da333444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans ROC-AUC: 0.663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# compute distance to nearest cluster center\n",
    "dist = np.min(kmeans.transform(X_scaled), axis=1)\n",
    "\n",
    "# flag high-distance points as anomalies\n",
    "threshold = np.percentile(dist, 97)  # top 3% farthest points\n",
    "y_pred_kmeans = (dist > threshold).astype(int)\n",
    "\n",
    "# invert\n",
    "auc = roc_auc_score(y_true, -dist)\n",
    "print(f\"KMeans ROC-AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9681818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Class SVM ROC-AUC: 0.537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "ocsvm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.05)\n",
    "ocsvm.fit(X_scaled)\n",
    "scores = -ocsvm.decision_function(X_scaled)\n",
    "auc = roc_auc_score(y_true, -scores)\n",
    "print(f\"One-Class SVM ROC-AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4869c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m526/526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step\n",
      "Autoencoder ROC-AUC: 0.635\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "inp = layers.Input(shape=(X_scaled.shape[1],))\n",
    "enc = layers.Dense(32, activation='relu')(inp)\n",
    "enc = layers.Dense(16, activation='relu')(enc)\n",
    "dec = layers.Dense(32, activation='relu')(enc)\n",
    "out = layers.Dense(X_scaled.shape[1], activation='linear')(dec)\n",
    "autoencoder = models.Model(inp, out)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=30, batch_size=64, verbose=0)\n",
    "\n",
    "recon_error = np.mean((X_scaled - autoencoder.predict(X_scaled))**2, axis=1)\n",
    "auc = roc_auc_score(y_true, -recon_error)\n",
    "print(f\"Autoencoder ROC-AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "44c98d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6113450515193665"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.copod import COPOD\n",
    "\n",
    "model = COPOD()    # completely unsupervised probabilistic detector\n",
    "model.fit(X_scaled)\n",
    "scores = model.decision_function(X_scaled)\n",
    "roc_auc_score(y_true, -scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "375e905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembled ROC-AUC: 0.631\n"
     ]
    }
   ],
   "source": [
    "scores = (\n",
    "    -iso.decision_function(X_scaled) +\n",
    "    -ocsvm.decision_function(X_scaled) +\n",
    "    -ell.decision_function(X_scaled)\n",
    ") / 3\n",
    "auc = roc_auc_score(y_true, -scores)\n",
    "print(f\"Ensembled ROC-AUC: {auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
